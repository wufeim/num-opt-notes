6.1 The BFGS Method
=====================================

The BFGS method is named for its discoverers Broyden, Fletcher, Goldfarb, and Shanno.

We begin with the quadratic model of the objective function at the current iterate :math:`x_k`:

.. math::

  m_k(p) = f_k + \nabla f_k^\top p + \frac{1}{2} p^\top B_k p

Here :math:`B_k` is an :math:`n \times n` symmetric positive definite matrix that will be updated at every iteration. The minimizer :math:`p_k` of this model is given by

.. math::

  p_k = -B_k^{-1}\nabla f_k

and is used as the search direction. The new iterate is

.. math::

  x_{k+1} = x_k + \alpha_kp_k

where :math:`\alpha_k` is chosen to satisfy the Wolfe conditions.

Davidon proposed to update :math:`B_k` in a simple manner to account for the curvature measured during the most recent step. Given :math:`x_{k+1}`, we wish to construct a new quadratic model, of the form

.. math::

  m_{k+1}(p) = f_{k+1} + \nabla f_{k+1}^\top p + \frac{1}{2}p^\top B_{k+1}p

One resonable requirement of :math:`B_{k+1}` is that the gradient of :math:`m_{k+1}` should match the gradient of the objective function :math:`f` at :math:`x_k` and :math:`x_{k+1}`. Since :math:`\nabla m_{k+1}(0)` is precisely :math:`\nabla f_{k+1}`, we only care about the first condition, written by

.. math::

  \nabla m_{k+1}(-\alpha_kp_k) = \nabla f_{k+1} - \alpha_kB_{k+1}p_k = \nabla f_k

Then we obtain

.. math::

  B_{k+1}\alpha_k p_k = \nabla f_{k+1} - \nabla f_k

To simplify the notation, we define the vectors

.. math::

  s_k = x_{k+1} - x_k = \alpha_kp_k, \;\;\; y_k = \nabla f_{k+1} - \nabla f_k

So we have

.. math::

  B_{k+1}s_k = y_k

We refer to this formula as the **secant equation**.

The secant equation requires :math:`B_k` maps :math:`s_k` into :math:`y_k`. This is possible only if :math:`s_k` and :math:`y_k` satisfy the **curvature condition**

.. math::

  s_k^\top y_k > 0

This does not always hold especially for nonconvex functions. However, it is guaranteed to hold if we impose the Wolfe or strong Wolfe conditions on the line search.

To determine :math:`B_k` uniquely, we impose the additional condition that **among all symmetric matrices satisfying the secant equation, :math:`B_k` is, in some sense, closet to the current matrix :math:`B_k`**. In other words, we solve the problem

.. math::

  \min_B \; & \lVert B - B_k \rVert \\
  \text{subject to } \; & B = B^\top, \; Bs_k = y_k

where :math:`s_k` and :math:`y_k` satisfy the curvature condition and :math:`B_k` is symmtric and positive definite. A matrix norm that allows easy solution of the minimization problem and gives rise to a scale-invariant optimization method is the weighted Frobenius norm

.. math::

  \lVert A \rVert_W \equiv \lVert W^{1/2}AW^{1/2} \rVert_F

where :math:`\lVert C \rVert_F = \sum_{i=1}^n \sum_{j=1}^n c_{ij}^2`. The weight matrix can be chosen as any matrix satisfying the relation :math:`Wy_k = s_k`. We assume that :math:`W = \bar{G}_k^{-1}` where :math:`\bar{G}_k` is the **average Hessian** defined by

.. math::

  \bar{G}_k = \left[ \int_0^1 \nabla^2 f(x_k + \tau\alpha_kp_k)d\tau \right]

The property

.. math::

  y_k = \bar{G}_k\alpha_kp_k = \bar{G}_ks_k

follows from Taylor's theorem. With this weighting matrix and this norm, the unique solution is

.. math::

  \text{(DFP)} \;\;\; B_{k+1} = (I - \rho_ky_ks_k^\top)B_k(I - \rho_ks_ky_k^\top) + \rho_ky_ky_k^\top

with

.. math::

  \rho_k = \frac{1}{y_k^\top s_k}

This formula is called the **DFP updating formula**.

The inverse of :math:`B_k`, denoted by :math:`H_k = B_k^{-1}` is useful in the implementation of the method. Using the Shereman-Morrison-Woodbury formula, we can derive the update of the inverse Hessian approximation :math:`H_k`

.. math::

  \text{(DFP)} \;\;\; H_{k+1} = H_k - \frac{H_ky_ky_k^\top H_k}{y_k^\top H_ky_K} + \frac{s_ks_k^\top}{y_k^\top s_k}

The DFP updating formula was soon superseded by the BFGS formula. Instead of imposing conditions on :math:`B_k`, we impose similar conditions on their inverses :math:`H_k`. The secant condition is now written as

.. math::

  H_{k+1}y_k = s_k

The condition of closeness is given by

.. math::

  \min_H \; & \lVert H - H_k \rVert \\
  \text{subject to } \; & H = H^\top, Hy_k = s_k

The unique solution :math:`H_{k+1}` is given by

.. math::

  \text{(BFGS)} \;\;\; H_{k+1} = (I - \rho_ks_ky_k^\top)H_k(I - \rho_ky_ks_k^\top) + \rho_ks_ks_k^\top

with

.. math::

  \rho_k = \frac{1}{y_k^\top s_k}

| **Algorithm 6.1** (BFGS Method).
|   Given starting point :math:`x_0`, convergence tolerance :math:`\epsilon > 0`, inverse Hessian approximation :math:`H_0`;
|   :math:`k \leftarrow 0`;
|   **while** :math:`\lVert \nabla f_k \rVert > \epsilon`;
|     Compute search direction :math:`p_k = -H_k\nabla f_k`;
|     Set :math:`x_{k+1} = x_k + \alpha_kp_k` where :math:`\alpha_k` is computed from a line search procedure to satisfy the Wolfe conditions;
|     Define :math:`s_k = x_{k+1} - x_k` and :math:`y_k = \nabla f_{k+1} - \nabla f_k`;
|     Compute :math:`H_{k+1}` with :math:`H_{k+1} = (I - \rho_ks_ky_k^\top)H_k(I - \rho_ky_ks_k^\top) + \rho_ks_ks_k^\top`;
|     :math:`k \leftarrow k + 1`;
|   **end while**

Each iteration can be performed at a cost of :math:`O(n^2)` arithmetic operations and the rate of convergence is superlinear, which is fast enough for most practical purposes.

We can derive a version of the BFGS algorithm that works with the Hessian approximation :math:`B_k` rather than :math:`H_k`. With Sherman-Morrison-Woodbury we have

.. math::

  \text{(BFGS)} \;\;\; B_{k+1} = B_k - \frac{B_ks_ks_k^\topB_k}{s_k^\top B_ks_k} + \frac{y_ky_k^\top}{y_k^\top s_k}

Properties of The BFGS Method
-------------------------------------

implementation
-------------------------------------
