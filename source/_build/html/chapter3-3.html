
<!DOCTYPE html>

<html lang="cn">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>3.3 Rate of Convergence &#8212; Notes for Numerical Optimization 0.1 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="3.4 Newton&#39;s Method with Hessian Modification" href="chapter3-4.html" />
    <link rel="prev" title="3.2 Convergence of Line Search Methods" href="chapter3-2.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="chapter3-4.html" title="3.4 Newton&#39;s Method with Hessian Modification"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="chapter3-2.html" title="3.2 Convergence of Line Search Methods"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Notes for Numerical Optimization 0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="chapter-3.html" accesskey="U">3 Line Search Methods</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">3.3 Rate of Convergence</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="rate-of-convergence">
<h1>3.3 Rate of Convergence<a class="headerlink" href="#rate-of-convergence" title="Permalink to this headline">¶</a></h1>
<p>Algorithmic strategies that achieve rapid convergence can sometimes conflict with the requirements of global convergence, and vice versa. The challenge is to design algorithms that incorporate both properties: good global convergence guarantees and a rapid rate of convergence.</p>
<div class="section" id="convergence-rate-of-steepest-descent">
<h2>Convergence Rate of Steepest Descent<a class="headerlink" href="#convergence-rate-of-steepest-descent" title="Permalink to this headline">¶</a></h2>
<p>Let us suppose that</p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{1}{2} x^\top Q x - b^\top x\]</div>
<p>where <span class="math notranslate nohighlight">\(Q\)</span> is symmetric and positive definite. The gradient is given by <span class="math notranslate nohighlight">\(\nabla f(x) = Qx - b\)</span> and the minimizer <span class="math notranslate nohighlight">\(x^*\)</span> is the unique solution of the linear system <span class="math notranslate nohighlight">\(Qx = b\)</span>.</p>
<p>It is easy to compute the step length <span class="math notranslate nohighlight">\(\alpha_k\)</span> that minimizes <span class="math notranslate nohighlight">\(f(x_k - \alpha \nabla f_k)\)</span>. By differentiating the funcion <span class="math notranslate nohighlight">\(f(x_k - \alpha \nabla f_k)\)</span> w.r.t. <span class="math notranslate nohighlight">\(\alpha\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}&amp; (-\nabla f_k)^\top Q (x_k - \alpha \nabla f_k) + b^\top \nabla f_k = 0 \\
&amp; -\nabla f_k^\top Q x_k + \alpha \nabla f_k^\top Q \nabla f_k + \nabla f_k^\top b = 0 \\
&amp; \alpha \nabla f_k^\top Q \nabla f_k - \nabla f_k^\top \nabla f_k = 0 \\
&amp; \alpha_k = \frac{\nabla f_k^\top \nabla f_k}{\nabla f_k^\top Q \nabla f_k}\end{split}\]</div>
<p>Then the steepest descent iteration is given by</p>
<div class="math notranslate nohighlight">
\[x_{k+1} = x_k - \left( \frac{\nabla f_k^\top \nabla f_k}{\nabla f_k^\top Q \nabla f_k} \right) \nabla f_k\]</div>
<p>which yields a closed-form expression for <span class="math notranslate nohighlight">\(x_{k+1}\)</span> in terms of <span class="math notranslate nohighlight">\(x_k\)</span>. In the figure below we plot a tpyical sequence of iterates generated by the steepest descent method on a two-dimensional quadratic objective function.</p>
<a class="reference internal image-reference" href="_images/fig3-2.png"><img alt="_images/fig3-2.png" src="_images/fig3-2.png" style="width: 320px;" /></a>
<p>To quantify the rate of convergence we introduce the weighted norm <span class="math notranslate nohighlight">\(\lVert x \rVert_Q^2 = x^\top Qx\)</span>. By using the relation <span class="math notranslate nohighlight">\(Qx^* = b\)</span>, we can show that</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{1}{2} \lVert x - x^* \rVert_Q^2 &amp; = \frac{1}{2} (x - x^*)^\top Q (x - x^*) \\
&amp; = \frac{1}{2} (x^\top Qx - x^\top Qx^* - x^{*\top}Qx + x^{*\top}Qx^*) \\
&amp; = \frac{1}{2} x^\top Qx - b^\top x + \frac{1}{2} b^\top x^{*\top} \\
&amp; = f(x) - f(x^*) \\
f(x^*) &amp; = \frac{1}{2} x^{*\top} Qx^* - b^\top x^* \\
&amp; = \frac{1}{2} x^{*\top} b - b^\top x^* \\
&amp; = - \frac{1}{2} b^\top x^*\end{split}\]</div>
<p>Noting that <span class="math notranslate nohighlight">\(\nabla f_k = Q(x_k - x^*)\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\lVert x_{k+1} - x^* \rVert_Q^2 = \left\{ 1 - \frac{(\nabla f_k^\top \nabla f_k)^2}{(\nabla f_k^\top Q \nabla f_k)(\nabla f_k^\top Q^{-1} \nabla f_k)} \right\} \lVert x_k - x^* \rVert_Q^2\]</div>
<p>(see Exercise 3.7). Since the term inside the brackets is hard to interpret, it is more useful to bound it in terms of condition number of the problem.</p>
<p><strong>Theorem 3.3.</strong> When the steepest descent method with exact line searches is applied to the strongly convex quadratic function, the error norm satisfies</p>
<div class="math notranslate nohighlight">
\[\lVert x_{k+1} - x^* \rVert_Q^2 \leq \left( \frac{\lambda_n - \lambda_1}{\lambda_n + \lambda_1} \right)^2 \lVert x_k - x^* \rVert_Q^2\]</div>
<p>where <span class="math notranslate nohighlight">\(0 &lt; \lambda_1 \leq \dots \leq \lambda_n\)</span> are the eigenvalues of <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<p>The proof of this theorem is given by Luenberger. This shows that the function values <span class="math notranslate nohighlight">\(f_k\)</span> converge to minimum <span class="math notranslate nohighlight">\(f_*\)</span> at a linear rate. In general, as the condition number <span class="math notranslate nohighlight">\(\kappa(Q) = \lambda_n / \lambda_1\)</span> increase, the contours of the quadratic become more elongated, the zigzagging becomes more pronounced, and the convergence degrades.</p>
<p><strong>Theorem 3.4.</strong> Suppose that <span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> is twice continuously differentiable, and that the iterates generated by the steepest-descent method with exact line searches converge to a point <span class="math notranslate nohighlight">\(x^*\)</span> at which the Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 f(x^*)\)</span> is positive definite. Let <span class="math notranslate nohighlight">\(r\)</span> be any scalar satisfying</p>
<div class="math notranslate nohighlight">
\[r \in \left( \frac{\lambda_n - \lambda_1}{\lambda_n + \lambda_1}, 1 \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_1 \leq \dots \leq \lambda_n\)</span> are the eigenvalues of <span class="math notranslate nohighlight">\(\nabla^2 f(x^*)\)</span>. Then for all <span class="math notranslate nohighlight">\(k\)</span> sufficiently large, we have</p>
<div class="math notranslate nohighlight">
\[f(x_{k+1}) - f(x^*) \leq r^2 [f(x_k) - f(x^*)]\]</div>
<p>Theorem 3.4 shows that the steepest descent method can have unacceptably slow rate of convergence, even when the Hessian is reasonably well conditioned.</p>
</div>
<div class="section" id="newton-s-method">
<h2>Newton's Method<a class="headerlink" href="#newton-s-method" title="Permalink to this headline">¶</a></h2>
<p>We now consider the Newton iteration, for which the search is given by</p>
<div class="math notranslate nohighlight">
\[p_k^N = - \nabla^2 f_k^{-1} \nabla f_k\]</div>
<p>Since the Hessian matrix <span class="math notranslate nohighlight">\(\nabla^2 f_k\)</span> may not always be positive definite, <span class="math notranslate nohighlight">\(p_k^N\)</span> may not always be a descent direction. In Section 3.4 and Chapter 4 we will discuss two approaches for obtaining a globally convergent iteration based on the Newton step: a line search approach, in which the Hessian is modified to make it positive definite, and a truth region approach, in which <span class="math notranslate nohighlight">\(\nabla^2 f_k\)</span> is used to form a quadratic model that is minimized in a ball around the current iterate <span class="math notranslate nohighlight">\(x_k\)</span>.</p>
<p>Here we disucss the local rate-of-convergence properties of Newton's method. For all <span class="math notranslate nohighlight">\(x\)</span> in the vicinity of a solution point <span class="math notranslate nohighlight">\(x^*\)</span> such that <span class="math notranslate nohighlight">\(\nabla^2 f(x^*)\)</span> is positive definite, the Hessian <span class="math notranslate nohighlight">\(\nabla^2 f(x)\)</span> will also be positive definite.</p>
<p><strong>Theorem 3.5.</strong> Suppose that <span class="math notranslate nohighlight">\(f\)</span> is twice differentiable and that the Hessian <span class="math notranslate nohighlight">\(\nabla^2 f(x)\)</span> is Lipschitz continuous in a neighborhood of a solution <span class="math notranslate nohighlight">\(x^*\)</span> at which the sufficient conditions are satisfied (Theorem 2.4). Consider the iteration <span class="math notranslate nohighlight">\(x_{k+1} = x_k + p_k\)</span> where <span class="math notranslate nohighlight">\(p_k^N = - \nabla^2 f_k^{-1} \nabla f_k\)</span>. Then</p>
<ol class="arabic simple">
<li><p>if the starting point <span class="math notranslate nohighlight">\(x_0\)</span> is sufficiently close to <span class="math notranslate nohighlight">\(x^*\)</span>, the sequence of iterates converges to <span class="math notranslate nohighlight">\(x^*\)</span></p></li>
<li><p>the rate of convergence of <span class="math notranslate nohighlight">\(\{x_k\}\)</span> is quadratic</p></li>
<li><p>the sequence of gradient norms <span class="math notranslate nohighlight">\(\{\lVert \nabla f_k \rVert\}\)</span> converges quadratically to zero</p></li>
</ol>
<p>As the iterates generated by Newton's method approach the solution, the Wolfe (or Goldstein) conditions will accept the step length :math:alpha_k = 1` for all large <span class="math notranslate nohighlight">\(k\)</span>. Implementations of Newton's method will set <span class="math notranslate nohighlight">\(\alpha_k = 1\)</span> for all large <span class="math notranslate nohighlight">\(k\)</span> and attain local quadratic rate of convergence.</p>
</div>
<div class="section" id="quasi-newton-methods">
<h2>Quasi-Newton Methods<a class="headerlink" href="#quasi-newton-methods" title="Permalink to this headline">¶</a></h2>
<p>Suppose now that the search direction has the form</p>
<div class="math notranslate nohighlight">
\[p_k = - B_k^{-1} \nabla f_k\]</div>
<p>where the symmetric and positive definite matrix <span class="math notranslate nohighlight">\(B_k\)</span> is updated at every iteration by a Quasi-Newton updating formula. We assume here that the step length <span class="math notranslate nohighlight">\(\alpha_k\)</span> is computed by an inexact line search that satisfies the Wolfe or strong Wolfe conditions, with the same proviso above: The line search algorithms will always try <span class="math notranslate nohighlight">\(\alpha = 1\)</span> first, and will accept this value if it satisfies the Wolfe conditions.</p>
<p><strong>Theorem 3.6.</strong> Suppose that <span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> is twice continuously differentiable. Consider the iteration <span class="math notranslate nohighlight">\(x_{k+1} = x_k + \alpha_k p_k\)</span>, where <span class="math notranslate nohighlight">\(p_k\)</span> is a descent direction and <span class="math notranslate nohighlight">\(\alpha_k\)</span> satisfies the Wolfe conditions with <span class="math notranslate nohighlight">\(c_1 \leq 1/2\)</span>. If the sequence <span class="math notranslate nohighlight">\(\{x_k\}\)</span> converges to a point <span class="math notranslate nohighlight">\(x^*\)</span> such that <span class="math notranslate nohighlight">\(\nabla f(x^*) = 0\)</span> and <span class="math notranslate nohighlight">\(\nabla^2 f(x^*)\)</span> is positive definite, and if the search direction satisfies</p>
<p>then</p>
<ol class="arabic simple">
<li><p>the step length <span class="math notranslate nohighlight">\(\alpha_k\)</span> is admissible for all <span class="math notranslate nohighlight">\(k\)</span> greater than a certain index <span class="math notranslate nohighlight">\(k_0\)</span></p></li>
<li><p>if <span class="math notranslate nohighlight">\(\alpha_k = 1\)</span> for all <span class="math notranslate nohighlight">\(k &gt; k_0\)</span>, <span class="math notranslate nohighlight">\(\{x_k\}\)</span> converges to <span class="math notranslate nohighlight">\(x^*\)</span> superlinearly</p></li>
</ol>
<p>If <span class="math notranslate nohighlight">\(p_k\)</span> is a quasi-Newton search direction, then <code class="xref eq docutils literal notranslate"><span class="pre">eq3.36</span></code> is equivalent to</p>
<div class="math notranslate nohighlight">
\[\lim_{k \to \infty} \frac{\lVert (B_k - \nabla^2 f(x^*))p_k \rVert}{\lVert p_k \rVert} = 0\]</div>
<p>Hence we have the result that a superlinear convergence rate can be attained even if the sequence of quasi-Newton matrices <span class="math notranslate nohighlight">\(B_k\)</span> does not converge to <span class="math notranslate nohighlight">\(\nabla^2 f(x^*)\)</span>. Importantly, this condition is both necessary and sufficient for the superlinear convergence of quasi-Newton methods.</p>
<p><strong>Theorem 3.7.</strong> Suppose that <span class="math notranslate nohighlight">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> is twice continuously differentiable. Consider the iteration <span class="math notranslate nohighlight">\(x_{k+1} = x_k + p_k\)</span> and that <span class="math notranslate nohighlight">\(p_k\)</span> is given by <span class="math notranslate nohighlight">\(p_k = - B_k^{-1} \nabla f_k\)</span>. Let us assume that <span class="math notranslate nohighlight">\(\{x_k\}\)</span> converges to a point <span class="math notranslate nohighlight">\(x^*\)</span> such that <span class="math notranslate nohighlight">\(\nabla f(x^*) = 0\)</span> and <span class="math notranslate nohighlight">\(\nabla^2 f(x^*)\)</span> is positive definite. Then <span class="math notranslate nohighlight">\(\{x_k\}\)</span> converges superlinearly if and only if <code class="xref eq docutils literal notranslate"><span class="pre">eq3.36</span></code> holds.</p>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">3.3 Rate of Convergence</a><ul>
<li><a class="reference internal" href="#convergence-rate-of-steepest-descent">Convergence Rate of Steepest Descent</a></li>
<li><a class="reference internal" href="#newton-s-method">Newton's Method</a></li>
<li><a class="reference internal" href="#quasi-newton-methods">Quasi-Newton Methods</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="chapter3-2.html"
                        title="previous chapter">3.2 Convergence of Line Search Methods</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="chapter3-4.html"
                        title="next chapter">3.4 Newton's Method with Hessian Modification</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/chapter3-3.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="chapter3-4.html" title="3.4 Newton&#39;s Method with Hessian Modification"
             >next</a> |</li>
        <li class="right" >
          <a href="chapter3-2.html" title="3.2 Convergence of Line Search Methods"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Notes for Numerical Optimization 0.1 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="chapter-3.html" >3 Line Search Methods</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">3.3 Rate of Convergence</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2020, Mofii.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.1.2.
    </div>
  </body>
</html>